import { createHash } from 'crypto'
import { createDeflate, createGzip } from 'zlib'

import { LoggerFactory, StructuredLogger } from '@repo/logs'

/**
 * Configuration options for archive creation and management
 */
export interface ArchiveConfig {
	/**
	 * Compression algorithm to use for archives
	 * @default 'gzip'
	 */
	compressionAlgorithm: 'gzip' | 'deflate' | 'none'

	/**
	 * Compression level (0-9, higher means more compression)
	 * @default 6
	 */
	compressionLevel: number

	/**
	 * Format for serializing records
	 * @default 'json'
	 */
	format: 'json' | 'jsonl' | 'parquet'

	/**
	 * Number of records to process in each batch
	 * @default 1000
	 */
	batchSize: number

	/**
	 * Whether to verify archive integrity after creation
	 * @default true
	 */
	verifyIntegrity: boolean

	/**
	 * Whether to encrypt archived data
	 * @default false
	 */
	encryptArchive: boolean
}

/**
 * Default configuration for archives
 */
export const DEFAULT_ARCHIVE_CONFIG: ArchiveConfig = {
	compressionAlgorithm: 'gzip',
	compressionLevel: 6,
	format: 'json',
	batchSize: 1000,
	verifyIntegrity: true,
	encryptArchive: false,
}

/**
 * Request parameters for retrieving archived data
 */
export interface ArchiveRetrievalRequest {
	/**
	 * Specific archive ID to retrieve
	 */
	archiveId?: string

	/**
	 * Principal ID to filter records by
	 */
	principalId?: string

	/**
	 * Organization ID to filter records by
	 */
	organizationId?: string

	/**
	 * Date range to filter records by
	 */
	dateRange?: {
		start: string
		end: string
	}

	/**
	 * Actions to filter records by
	 */
	actions?: string[]

	/**
	 * Data classifications to filter archives by
	 */
	dataClassifications?: string[]

	/**
	 * Retention policies to filter archives by
	 */
	retentionPolicies?: string[]

	/**
	 * Maximum number of records to return
	 * @default 100
	 */
	limit?: number

	/**
	 * Number of records to skip
	 * @default 0
	 */
	offset?: number
}

/**
 * Result of archive creation
 */
export interface ArchiveResult {
	/**
	 * Unique identifier for the archive
	 */
	archiveId: string

	/**
	 * Number of records archived
	 */
	recordCount: number

	/**
	 * Size of the original data in bytes
	 */
	originalSize: number

	/**
	 * Size of the compressed data in bytes
	 */
	compressedSize: number

	/**
	 * Compression ratio (compressed size / original size)
	 */
	compressionRatio: number

	/**
	 * Checksum of the original data
	 */
	checksumOriginal: string

	/**
	 * Checksum of the compressed data
	 */
	checksumCompressed: string

	/**
	 * Status of integrity verification
	 */
	verificationStatus: 'verified' | 'failed' | 'skipped'

	/**
	 * Timestamp when the archive was created
	 */
	timestamp: string

	/**
	 * Processing time in milliseconds
	 */
	processingTime: number
}

/**
 * Result of archive retrieval
 */
export interface ArchiveRetrievalResult {
	/**
	 * Unique identifier for the retrieval request
	 */
	requestId: string

	/**
	 * Timestamp when the data was retrieved
	 */
	retrievedAt: string

	/**
	 * Total number of records retrieved
	 */
	recordCount: number

	/**
	 * Total size of the retrieved data in bytes
	 */
	totalSize: number

	/**
	 * Time taken to retrieve the data in milliseconds
	 */
	retrievalTime: number

	/**
	 * Archives that matched the retrieval criteria
	 */
	archives: Array<{
		/**
		 * Archive metadata
		 */
		metadata: any

		/**
		 * Records from the archive
		 */
		records: any[]
	}>
}

/**
 * Result of archiving data by retention policies
 */
export interface RetentionArchiveResult {
	/**
	 * Retention policy name
	 */
	policy: string

	/**
	 * Number of records archived
	 */
	recordsArchived: number

	/**
	 * Number of records deleted
	 */
	recordsDeleted: number

	/**
	 * Size of the original data in bytes
	 */
	originalDataSize: number

	/**
	 * Size of the compressed data in bytes
	 */
	compressedDataSize: number

	/**
	 * Compression ratio (compressed size / original size)
	 */
	compressionRatio: number

	/**
	 * Time taken to process the data in milliseconds
	 */
	processingTime: number

	/**
	 * Status of integrity verification
	 */
	verificationStatus: 'verified' | 'failed' | 'skipped'

	/**
	 * Summary of archived data
	 */
	summary?: {
		/**
		 * Records by data classification
		 */
		byClassification: Record<string, number>

		/**
		 * Records by action type
		 */
		byAction: Record<string, number>
	}
}

/**
 * Result of secure data deletion
 */
export interface SecureDeleteResult {
	/**
	 * Number of records deleted
	 */
	recordsDeleted: number

	/**
	 * Status of deletion verification
	 */
	verificationStatus: 'verified' | 'failed' | 'skipped'

	/**
	 * Timestamp when the deletion was performed
	 */
	deletionTimestamp: string

	/**
	 * Details of verification if performed
	 */
	verificationDetails?: {
		/**
		 * Whether all records were successfully deleted
		 */
		allDeleted: boolean

		/**
		 * Number of records that still remain
		 */
		remainingRecords: number
	}
}

/**
 * Result of archive validation
 */
export interface ArchiveValidationResult {
	/**
	 * Total number of archives validated
	 */
	totalArchives: number

	/**
	 * Number of valid archives
	 */
	validArchives: number

	/**
	 * Number of corrupted archives
	 */
	corruptedArchives: number

	/**
	 * IDs of corrupted archives
	 */
	corruptedArchiveIds: string[]

	/**
	 * Timestamp when validation was performed
	 */
	validationTimestamp: string
}

/**
 * Archive statistics
 */
export interface ArchiveStatistics {
	/**
	 * Total number of archives
	 */
	totalArchives: number

	/**
	 * Total size of compressed archives in bytes
	 */
	totalCompressedSize: number

	/**
	 * Total size of original data in bytes
	 */
	totalOriginalSize: number

	/**
	 * Average compression ratio
	 */
	averageCompressionRatio: number

	/**
	 * Archives by retention policy
	 */
	archivesByPolicy: Record<string, number>

	/**
	 * Archives by data classification
	 */
	archivesByClassification: Record<string, number>

	/**
	 * Timestamp of oldest archive
	 */
	oldestArchive?: string

	/**
	 * Timestamp of newest archive
	 */
	newestArchive?: string
}

/**
 * Result of archive cleanup
 */
export interface ArchiveCleanupResult {
	/**
	 * Number of archives deleted
	 */
	archivesDeleted: number

	/**
	 * Amount of space freed in bytes
	 */
	spaceFreed: number

	/**
	 * Timestamp when cleanup was performed
	 */
	cleanupTimestamp: string
}

/**
 * Base class for audit data archival services
 * Provides core functionality for archiving, retrieving, and managing audit data
 */
export abstract class ArchivalService {
	protected db: any
	protected auditLogTable: any
	protected retentionPolicyTable: any
	protected archiveTable: any
	protected config: ArchiveConfig
	protected logger: StructuredLogger

	/**
	 * Create a new ArchivalService
	 *
	 * @param db Database instance
	 * @param auditLogTable Audit log table
	 * @param retentionPolicyTable Retention policy table
	 * @param archiveTable Archive storage table
	 * @param config Archive configuration
	 */
	constructor(
		db: any,
		auditLogTable: any,
		retentionPolicyTable: any,
		archiveTable: any,
		config: Partial<ArchiveConfig> = {}
	) {
		this.db = db
		this.auditLogTable = auditLogTable
		this.retentionPolicyTable = retentionPolicyTable
		this.archiveTable = archiveTable
		this.config = { ...DEFAULT_ARCHIVE_CONFIG, ...config }
		LoggerFactory.setDefaultConfig({
			level: (process.env.LOG_LEVEL || 'info') as 'debug' | 'info' | 'warn' | 'error',
			enablePerformanceLogging: true,
			enableErrorTracking: true,
			enableMetrics: false,
			format: 'json',
			outputs: ['otpl'],
			otplConfig: {
				endpoint: 'http://localhost:5080/api/default/default/_json',
				headers: {
					Authorization: process.env.OTLP_AUTH_HEADER || '',
				},
			},
		})

		this.logger = LoggerFactory.createLogger({
			service: '@repo/audit - ArchivalService',
		})
	}

	/**
	 * Archive data based on active retention policies
	 *
	 * @returns Array of results for each policy processed
	 */
	public async archiveDataByRetentionPolicies(): Promise<RetentionArchiveResult[]> {
		const results: RetentionArchiveResult[] = []

		try {
			// Get active retention policies
			const policies = await this.db
				.select()
				.from(this.retentionPolicyTable)
				.where({ isActive: 'true' })

			// Process each policy
			for (const policy of policies) {
				try {
					const startTime = Date.now()
					const cutoffDate = new Date()
					cutoffDate.setDate(cutoffDate.getDate() - (policy.archiveAfterDays || 90))

					// Find records to archive
					const records = await this.db
						.select()
						.from(this.auditLogTable)
						.where({
							dataClassification: policy.dataClassification,
							retentionPolicy: policy.policyName,
							archivedAt: null,
						})
						.where('timestamp', '<', cutoffDate.toISOString())

					if (records.length === 0) {
						continue
					}

					// Create summary
					const summary = {
						byClassification: {} as Record<string, number>,
						byAction: {} as Record<string, number>,
					}

					// Count by classification and action
					for (const record of records) {
						const classification = record.dataClassification || 'UNKNOWN'
						const action = record.action || 'UNKNOWN'

						summary.byClassification[classification] =
							(summary.byClassification[classification] || 0) + 1
						summary.byAction[action] = (summary.byAction[action] || 0) + 1
					}

					// Create archive
					const archiveResult = await this.createArchive(records, {
						retentionPolicy: policy.policyName,
						dataClassification: policy.dataClassification,
						dateRange: {
							start: records[0].timestamp,
							end: records[records.length - 1].timestamp,
						},
					})

					// Mark records as archived
					const now = new Date().toISOString()
					await this.db
						.update(this.auditLogTable)
						.set({ archivedAt: now })
						.where({ retentionPolicy: policy.policyName })
						.where('timestamp', '<', cutoffDate.toISOString())

					// Delete records if deleteAfterDays is specified and not null
					let recordsDeleted = 0
					if (policy.deleteAfterDays !== null && policy.deleteAfterDays !== undefined) {
						const deleteCutoffDate = new Date()
						deleteCutoffDate.setDate(deleteCutoffDate.getDate() - policy.deleteAfterDays)

						const deleteResult = await this.db
							.delete(this.auditLogTable)
							.where({ retentionPolicy: policy.policyName })
							.where('timestamp', '<', deleteCutoffDate.toISOString())

						recordsDeleted = deleteResult.rowCount || 0
					}

					const processingTime = Date.now() - startTime

					// Add result
					results.push({
						policy: policy.policyName,
						recordsArchived: records.length,
						recordsDeleted,
						originalDataSize: archiveResult.originalSize,
						compressedDataSize: archiveResult.compressedSize,
						compressionRatio: archiveResult.compressionRatio,
						processingTime,
						verificationStatus: archiveResult.verificationStatus,
						summary,
					})
				} catch (error) {
					this.logger.error(
						`Error processing policy ${policy.policyName}:`,
						error instanceof Error ? error : 'Unknown error during retention policy processing'
					)
					// Continue with next policy
				}
			}
		} catch (error) {
			this.logger.error(
				'Error retrieving retention policies:',
				error instanceof Error ? error : 'Unknown error during retention policy processing'
			)
		}

		return results
	}

	/**
	 * Create an archive of audit records
	 *
	 * @param records Records to archive
	 * @param metadata Metadata for the archive
	 * @returns Archive result
	 */
	public async createArchive(
		records: any[],
		metadata: {
			retentionPolicy: string
			dataClassification: string
			dateRange?: { start: string; end: string }
		}
	): Promise<ArchiveResult> {
		const startTime = Date.now()
		const archiveId = `archive-${Date.now()}-${Math.random().toString(36).substring(2, 15)}`

		try {
			// Serialize records based on format
			let serializedData: string
			switch (this.config.format) {
				case 'json':
					serializedData = JSON.stringify(records)
					break
				case 'jsonl':
					serializedData = records.map((record) => JSON.stringify(record)).join('\\n')
					break
				default:
					this.logger.error(
						'Unsupported archive format',
						`Unsupported archive format: ${this.config.format}`
					)
					throw new Error(`Unsupported archive format: ${this.config.format}`)
			}

			// Calculate original checksum
			const checksumOriginal = createHash('sha256').update(serializedData).digest('hex')
			const originalSize = Buffer.byteLength(serializedData, 'utf8')

			// Compress data
			let compressedData: Buffer
			let checksumCompressed: string

			switch (this.config.compressionAlgorithm) {
				case 'gzip': {
					compressedData = await new Promise((resolve, reject) => {
						const gzip = createGzip({ level: this.config.compressionLevel })
						const chunks: Buffer[] = []
						gzip.on('data', (chunk) => chunks.push(chunk))
						gzip.on('end', () => resolve(Buffer.concat(chunks)))
						gzip.on('error', reject)
						gzip.end(Buffer.from(serializedData, 'utf8'))
					})
					break
				}
				case 'deflate': {
					compressedData = await new Promise((resolve, reject) => {
						const deflate = createDeflate({ level: this.config.compressionLevel })
						const chunks: Buffer[] = []
						deflate.on('data', (chunk) => chunks.push(chunk))
						deflate.on('end', () => resolve(Buffer.concat(chunks)))
						deflate.on('error', reject)
						deflate.end(Buffer.from(serializedData, 'utf8'))
					})
					break
				}
				case 'none':
					compressedData = Buffer.from(serializedData, 'utf8')
					break
				default:
					this.logger.error(
						'Unsupported compression algorithm',
						`Unsupported compression algorithm: ${this.config.compressionAlgorithm}`
					)
					throw new Error(`Unsupported compression algorithm: ${this.config.compressionAlgorithm}`)
			}

			// Calculate compressed checksum
			checksumCompressed = createHash('sha256').update(compressedData).digest('hex')
			const compressedSize = compressedData.length

			// Create archive metadata
			const archiveMetadata = {
				archiveId,
				createdAt: new Date().toISOString(),
				recordCount: records.length,
				originalSize,
				compressedSize,
				compressionRatio: compressedSize / originalSize,
				checksumOriginal,
				checksumCompressed,
				...metadata,
				config: {
					compressionAlgorithm: this.config.compressionAlgorithm,
					compressionLevel: this.config.compressionLevel,
					format: this.config.format,
					batchSize: this.config.batchSize,
					verifyIntegrity: this.config.verifyIntegrity,
					encryptArchive: this.config.encryptArchive,
				},
			}

			// Create archive object
			const archive = {
				id: archiveId,
				metadata: archiveMetadata,
				data: compressedData,
				createdAt: archiveMetadata.createdAt,
				retrievedCount: 0,
				lastRetrievedAt: undefined,
			}

			// Store archive
			await this.storeArchive(archive)

			// Verify integrity if configured
			let verificationStatus: 'verified' | 'failed' | 'skipped' = 'skipped'
			if (this.config.verifyIntegrity) {
				const isValid = await this.verifyArchiveIntegrity(archiveId)
				verificationStatus = isValid ? 'verified' : 'failed'
			}

			const processingTime = Date.now() - startTime

			return {
				archiveId,
				recordCount: records.length,
				originalSize,
				compressedSize,
				compressionRatio: compressedSize / originalSize,
				checksumOriginal,
				checksumCompressed,
				verificationStatus,
				timestamp: archiveMetadata.createdAt,
				processingTime,
			}
		} catch (error) {
			const err =
				error instanceof Error ? error : new Error('Unknown error during archive creation')
			this.logger.error('Error creating archive:', err)
			throw err
		}
	}

	/**
	 * Retrieve archived data based on criteria
	 *
	 * @param request Retrieval request parameters
	 * @returns Retrieved data
	 */
	public async retrieveArchivedData(
		request: ArchiveRetrievalRequest
	): Promise<ArchiveRetrievalResult> {
		const startTime = Date.now()
		const requestId = `retrieval-${Date.now()}-${Math.random().toString(36).substring(2, 15)}`

		try {
			// Find matching archives
			const archives = await this.findMatchingArchives(request)

			// Process each archive
			const processedArchives = []
			let totalRecords = 0
			let totalSize = 0

			for (const archive of archives) {
				// Decompress archive data
				const decompressedData = await this.decompressArchiveData(archive)

				// Parse records
				let records: any[]
				switch (archive.metadata.config?.format || 'json') {
					case 'json':
						records = JSON.parse(decompressedData)
						break
					case 'jsonl':
						records = decompressedData
							.split('\\n')
							.filter((line) => line.trim())
							.map((line) => JSON.parse(line))
						break
					default:
						this.logger.error(
							`Unsupported archive format: ${archive.metadata.config?.format}`,
							`Unsupported archive format: ${archive.metadata.config?.format}`
						)
						throw new Error(`Unsupported archive format: ${archive.metadata.config?.format}`)
				}

				// Filter records based on request criteria
				const filteredRecords = this.filterRecords(records, request)

				if (filteredRecords.length > 0) {
					processedArchives.push({
						metadata: archive.metadata,
						records: filteredRecords,
					})

					totalRecords += filteredRecords.length
					totalSize += Buffer.byteLength(JSON.stringify(filteredRecords), 'utf8')

					// Update retrieval statistics
					await this.updateRetrievalStatistics(archive.id)
				}
			}

			const retrievalTime = Date.now() - startTime

			return {
				requestId,
				retrievedAt: new Date().toISOString(),
				recordCount: totalRecords,
				totalSize,
				retrievalTime,
				archives: processedArchives,
			}
		} catch (error) {
			const err = error instanceof Error ? error : new Error('Unknown error during data retrieval')
			this.logger.error('Error retrieving archived data:', err)
			throw err
		}
	}

	/**
	 * Securely delete audit data based on criteria
	 *
	 * @param criteria Deletion criteria
	 * @returns Deletion result
	 */
	public async secureDeleteData(criteria: {
		principalId?: string
		organizationId?: string
		dateRange?: { start: string; end: string }
		dataClassifications?: string[]
		retentionPolicies?: string[]
		verifyDeletion?: boolean
	}): Promise<SecureDeleteResult> {
		try {
			// Build query to find records to delete
			let query = this.db.select().from(this.auditLogTable)

			if (criteria.principalId) {
				query = query.where({ principalId: criteria.principalId })
			}

			if (criteria.organizationId) {
				query = query.where({ organizationId: criteria.organizationId })
			}

			if (criteria.dateRange) {
				query = query.where('timestamp', '>=', criteria.dateRange.start)
				query = query.where('timestamp', '<=', criteria.dateRange.end)
			}

			if (criteria.dataClassifications && criteria.dataClassifications.length > 0) {
				query = query.where('dataClassification', 'in', criteria.dataClassifications)
			}

			if (criteria.retentionPolicies && criteria.retentionPolicies.length > 0) {
				query = query.where('retentionPolicy', 'in', criteria.retentionPolicies)
			}

			// Get records to delete
			const recordsToDelete = await query

			if (recordsToDelete.length === 0) {
				return {
					recordsDeleted: 0,
					verificationStatus: 'skipped',
					deletionTimestamp: new Date().toISOString(),
				}
			}

			// Extract IDs for verification
			const recordIds = recordsToDelete.map((record: any) => record.id)

			// Delete records
			const deleteResult = await this.db.delete(this.auditLogTable).where('id', 'in', recordIds)

			const recordsDeleted = deleteResult.rowCount || 1

			// Verify deletion if requested
			let verificationStatus: 'verified' | 'failed' | 'skipped' = 'skipped'
			let verificationDetails

			if (criteria.verifyDeletion) {
				// Check if any records still exist
				const remainingRecords = []

				for (const id of recordIds) {
					const record = await this.db.select().from(this.auditLogTable).where({ id }).limit(1)

					if (record.length > 0) {
						remainingRecords.push(record[0])
					}
				}

				const allDeleted = remainingRecords.length === 0
				verificationStatus = allDeleted ? 'verified' : 'failed'
				verificationDetails = {
					allDeleted,
					remainingRecords: remainingRecords.length,
				}
			}

			return {
				recordsDeleted,
				verificationStatus,
				deletionTimestamp: new Date().toISOString(),
				verificationDetails,
			}
		} catch (error) {
			const err = error instanceof Error ? error : new Error('Unknown error during secure deletion')
			this.logger.error('Error during secure deletion:', err)
			throw err
		}
	}

	/**
	 * Get statistics about archived data
	 *
	 * @returns Archive statistics
	 */
	public async getArchiveStatistics(): Promise<ArchiveStatistics> {
		try {
			// Get all archives
			const archives = await this.db.select().from(this.archiveTable)

			if (archives.length === 0) {
				return {
					totalArchives: 0,
					totalCompressedSize: 0,
					totalOriginalSize: 0,
					averageCompressionRatio: 0,
					archivesByPolicy: {},
					archivesByClassification: {},
				}
			}

			// Calculate statistics
			let totalCompressedSize = 0
			let totalOriginalSize = 0
			const archivesByPolicy: Record<string, number> = {}
			const archivesByClassification: Record<string, number> = {}
			let oldestDate: string | undefined
			let newestDate: string | undefined

			for (const archive of archives) {
				// Size statistics
				totalCompressedSize += archive.metadata.compressedSize || 0
				totalOriginalSize += archive.metadata.originalSize || 0

				// Policy statistics
				const policy = archive.metadata.retentionPolicy || 'unknown'
				archivesByPolicy[policy] = (archivesByPolicy[policy] || 0) + 1

				// Classification statistics
				const classification = archive.metadata.dataClassification || 'unknown'
				archivesByClassification[classification] =
					(archivesByClassification[classification] || 0) + 1

				// Date statistics
				const createdAt = archive.createdAt || archive.metadata.createdAt
				if (!oldestDate || createdAt < oldestDate) {
					oldestDate = createdAt
				}
				if (!newestDate || createdAt > newestDate) {
					newestDate = createdAt
				}
			}

			// Calculate average compression ratio
			const averageCompressionRatio =
				totalOriginalSize > 0 ? totalCompressedSize / totalOriginalSize : 0

			return {
				totalArchives: archives.length,
				totalCompressedSize,
				totalOriginalSize,
				averageCompressionRatio,
				archivesByPolicy,
				archivesByClassification,
				oldestArchive: oldestDate,
				newestArchive: newestDate,
			}
		} catch (error) {
			const err =
				error instanceof Error ? error : new Error('Unknown error getting archive statistics')
			this.logger.error('Error getting archive statistics:', err)
			throw err
		}
	}

	/**
	 * Clean up old archives based on retention policies
	 *
	 * @returns Cleanup result
	 */
	public async cleanupOldArchives(): Promise<ArchiveCleanupResult> {
		try {
			// Get retention policies with delete rules
			const policies = await this.db
				.select()
				.from(this.retentionPolicyTable)
				.where({ isActive: 'true' })
				.where('deleteAfterDays', 'is not', null)

			let archivesDeleted = 0
			let spaceFreed = 0

			// Process each policy
			for (const policy of policies) {
				const cutoffDate = new Date()
				cutoffDate.setDate(cutoffDate.getDate() - policy.deleteAfterDays)

				// Find archives to delete
				const archivesToDelete = await this.db
					.select()
					.from(this.archiveTable)
					.where('metadata->retentionPolicy', '=', policy.policyName)
					.where('createdAt', '<', cutoffDate.toISOString())

				// Delete archives
				if (archivesToDelete.length > 0) {
					// Track space freed
					for (const archive of archivesToDelete) {
						spaceFreed += archive.metadata.compressedSize || 0
					}

					// Delete archives
					const archiveIds = archivesToDelete.map((archive: any) => archive.id)
					const deleteResult = await this.db.delete(this.archiveTable).where('id', 'in', archiveIds)

					archivesDeleted += deleteResult.rowCount || 0
				}
			}

			return {
				archivesDeleted,
				spaceFreed,
				cleanupTimestamp: new Date().toISOString(),
			}
		} catch (error) {
			const err =
				error instanceof Error ? error : new Error('Unknown error during cleanup of old archives')
			this.logger.error('Error cleaning up old archives:', err)
			throw err
		}
	}

	/**
	 * Validate integrity of all archives
	 *
	 * @returns Validation result
	 */
	public async validateAllArchives(): Promise<ArchiveValidationResult> {
		try {
			// Get all archives
			const archives = await this.db.select().from(this.archiveTable)

			let validArchives = 0
			let corruptedArchives = 0
			const corruptedArchiveIds: string[] = []

			// Validate each archive
			for (const archive of archives) {
				try {
					const isValid = await this.verifyArchiveIntegrity(archive.id)

					if (isValid) {
						validArchives++
					} else {
						corruptedArchives++
						corruptedArchiveIds.push(archive.id)
					}
				} catch (error) {
					// Count as corrupted if verification fails
					corruptedArchives++
					corruptedArchiveIds.push(archive.id)
				}
			}

			return {
				totalArchives: archives.length,
				validArchives,
				corruptedArchives,
				corruptedArchiveIds,
				validationTimestamp: new Date().toISOString(),
			}
		} catch (error) {
			const err =
				error instanceof Error
					? error
					: new Error('Unknown error during validation of all archives')
			this.logger.error('Error validating all archives:', err)
			throw err
		}
	}

	/**
	 * Verify the integrity of an archive
	 *
	 * @param archiveId ID of the archive to verify
	 * @returns Whether the archive is valid
	 */
	protected async verifyArchiveIntegrity(archiveId: string): Promise<boolean> {
		try {
			// Get archive
			const archive = await this.getArchiveById(archiveId)

			if (!archive) {
				return false
			}

			// Verify compressed data checksum
			const actualCompressedChecksum = createHash('sha256').update(archive.data).digest('hex')
			const expectedCompressedChecksum = archive.metadata.checksumCompressed

			if (actualCompressedChecksum !== expectedCompressedChecksum) {
				return false
			}

			// Decompress data
			const decompressedData = await this.decompressArchiveData(archive)

			// Verify original data checksum
			const actualOriginalChecksum = createHash('sha256').update(decompressedData).digest('hex')
			const expectedOriginalChecksum = archive.metadata.checksumOriginal

			return actualOriginalChecksum === expectedOriginalChecksum
		} catch (error) {
			const err =
				error instanceof Error
					? error
					: new Error('Unknown error during archive integrity verification')
			this.logger.error(`Error verifying archive integrity for ${archiveId}:`, err)
			return false
		}
	}

	/**
	 * Filter records based on retrieval criteria
	 *
	 * @param records Records to filter
	 * @param request Retrieval request
	 * @returns Filtered records
	 */
	protected filterRecords(records: any[], request: ArchiveRetrievalRequest): any[] {
		return records.filter((record) => {
			// Filter by principal ID
			if (request.principalId && record.principalId !== request.principalId) {
				return false
			}

			// Filter by actions
			if (
				request.actions &&
				request.actions.length > 0 &&
				!request.actions.includes(record.action)
			) {
				return false
			}

			// Filter by date range
			if (request.dateRange) {
				const timestamp = new Date(record.timestamp).getTime()
				const start = new Date(request.dateRange.start).getTime()
				const end = new Date(request.dateRange.end).getTime()

				if (timestamp < start || timestamp > end) {
					return false
				}
			}

			// Filter by data classification
			if (
				request.dataClassifications &&
				request.dataClassifications.length > 0 &&
				!request.dataClassifications.includes(record.dataClassification)
			) {
				return false
			}

			return true
		})
	}

	/**
	 * Store an archive in the database
	 *
	 * @param archive Archive to store
	 */
	protected abstract storeArchive(archive: any): Promise<void>

	/**
	 * Get an archive by ID
	 *
	 * @param archiveId Archive ID
	 * @returns Archive or null if not found
	 */
	protected abstract getArchiveById(archiveId: string): Promise<any | null>

	/**
	 * Find archives matching retrieval criteria
	 *
	 * @param request Retrieval request
	 * @returns Matching archives
	 */
	protected abstract findMatchingArchives(request: ArchiveRetrievalRequest): Promise<any[]>

	/**
	 * Decompress archive data
	 *
	 * @param archive Archive to decompress
	 * @returns Decompressed data
	 */
	protected abstract decompressArchiveData(archive: any): Promise<string>

	/**
	 * Update retrieval statistics for an archive
	 *
	 * @param archiveId Archive ID
	 */
	protected abstract updateRetrievalStatistics(archiveId: string): Promise<void>
}
